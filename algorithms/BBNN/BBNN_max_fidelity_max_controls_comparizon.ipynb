{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBNN-QC\n",
    "\n",
    "Neural Network with time as input and controls as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from qutip import qutrit_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sin(nn.Module):\n",
    "    \"\"\"The sin activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializer method.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_):\n",
    "        return torch.sin(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, hidden_size, nb_hidden_layers = 2, input_size = 1, output_size = 1, activation_fn = nn.Tanh, max_control = 1.):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        self.nb_hidden_layers = nb_hidden_layers\n",
    "        self.hidden_layers = []\n",
    "        self.hidden_act_layers = []\n",
    "        self.max_control = max_control\n",
    "\n",
    "        # input layer\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.relu_input = activation_fn()\n",
    "\n",
    "        # hidden layers\n",
    "        for layer in range(nb_hidden_layers):\n",
    "            new_layer = nn.Linear(hidden_size, hidden_size)\n",
    "            self.hidden_layers.append(new_layer)\n",
    "            self.hidden_act_layers.append(activation_fn())\n",
    "\n",
    "            # hidden layer parameters should be registered\n",
    "            self.register_parameter(f\"weights_{layer}\", new_layer.weight)\n",
    "            self.register_parameter(f\"bias_{layer}\", new_layer.bias)\n",
    "\n",
    "        # output layer\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.input_layer(x)\n",
    "        out = self.relu_input(out)\n",
    "\n",
    "        for layer in range(self.nb_hidden_layers):\n",
    "            out = self.hidden_layers[layer](out)\n",
    "            out = self.hidden_act_layers[layer](out)\n",
    "\n",
    "        out = self.output_layer(out)\n",
    "        out = torch.clip(input=out, min=-self.max_control, max=self.max_control)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define discretized time, initialize Neural net and Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timings = [(2.4e-3, 2.4, 1e3, 1.), (1.4e-3, 1.4, 1e3, 2.), (1.2e-3, 1.2, 1e3, 3.), (1.1e-3, 1.1, 1e3, 4.)]\n",
    "print_interval = 100\n",
    "ξ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concurrence(state):\n",
    "    c1 = state[0]\n",
    "    c2 = state[1]\n",
    "    c3 = state[2]\n",
    "\n",
    "    return torch.abs(2* c1 * c3 - c2**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define the objective function, it evolves quantum system in time based on current NN outputs and returns the final fidelity of the terminal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_fidelity_custom(u_pred, time, Dt):\n",
    "    [proj1, _, proj3, trans12, trans23, _] = qutrit_ops()\n",
    "    trans21 = trans12.dag()\n",
    "    trans32 = trans23.dag()\n",
    "\n",
    "    Ω_pred = u_pred[:, 0:1]\n",
    "    Δ_pred = u_pred[:, 1:2]\n",
    "\n",
    "    time_tensor = torch.from_numpy(time).reshape(len(time), 1)\n",
    "\n",
    "    current_state = torch.from_numpy(np.array([1. + 0j, 0, 0]))\n",
    "    final_state = torch.from_numpy(np.array([0. + 0j, 1., 0]))\n",
    "    \n",
    "    H1 = (trans12 + trans21 + trans23 + trans32)\n",
    "\n",
    "    for i in range(len(time_tensor)):\n",
    "        H = torch.from_numpy(H1.full()) * Ω_pred[i] / np.sqrt(2)\n",
    "        H = H + torch.from_numpy(proj3.full()) * (4 * ξ - Δ_pred[i])\n",
    "        H = H + torch.from_numpy(proj1.full()) * (Δ_pred[i])\n",
    "        \n",
    "        current_state = torch.matmul(torch.linalg.matrix_exp(-1j * H * Dt), current_state)\n",
    "\n",
    "    infidelity = 1 - abs(torch.inner(current_state, final_state)) ** 2\n",
    "\n",
    "    return infidelity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define function to get the quantum state at all intermediate time steps using the optimal controls discovered by the NN after the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_states(u_pred, time, Dt):\n",
    "    [proj1, _, proj3, trans12, trans23, _] = qutrit_ops()\n",
    "    trans21 = trans12.dag()\n",
    "    trans32 = trans23.dag()\n",
    "\n",
    "    Ω_pred = u_pred[:, 0:1]\n",
    "    Δ_pred = u_pred[:, 1:2]\n",
    "\n",
    "    time_tensor = torch.from_numpy(time).reshape(len(time), 1)\n",
    "\n",
    "    states = []\n",
    "\n",
    "    current_state = torch.from_numpy(np.array([1. + 0j, 0, 0]))\n",
    "    final_state = torch.from_numpy(np.array([0. + 0j, 1., 0]))\n",
    "    \n",
    "    H1 = (trans12 + trans21 + trans23 + trans32)\n",
    "\n",
    "    for i in range(len(time_tensor)):\n",
    "        H = torch.from_numpy(H1.full()) * Ω_pred[i] / np.sqrt(2)\n",
    "        H = H + torch.from_numpy(proj3.full()) * (4 * ξ - Δ_pred[i])\n",
    "        H = H + torch.from_numpy(proj1.full()) * (Δ_pred[i])\n",
    "        \n",
    "        current_state = torch.matmul(torch.linalg.matrix_exp(-1j * H * Dt), current_state)\n",
    "        states.append(current_state)\n",
    "\n",
    "    infidelity = 1 - abs(torch.inner(current_state, final_state)) ** 2\n",
    "\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training loop of the Deep NN using the defined objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_losses = []\n",
    "fidelities_array = []\n",
    "populations_array = []\n",
    "controls_array = []\n",
    "timings_array = []\n",
    "max_control_array = []\n",
    "final_states_array = []\n",
    "\n",
    "torch.manual_seed(986532)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (Dt, T, N, max_control) in timings:\n",
    "    max_control_array.append(max_control)\n",
    "    losses = []\n",
    "    loss_threshold = 1e-4\n",
    "    loss_float = 1\n",
    "    iterations = 10000\n",
    "\n",
    "    time = np.arange(0, T + Dt, Dt, dtype = np.float32)\n",
    "    time_tensor = torch.from_numpy(time).reshape(len(time), 1)\n",
    "    time_tensor.requires_grad_(True)\n",
    "\n",
    "    timings_array.append(time)\n",
    "\n",
    "    # create model\n",
    "    model = NeuralNet(hidden_size=75, input_size=1, output_size=2, nb_hidden_layers=4, activation_fn=nn.Tanh, max_control=max_control)\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    num_epochs = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # while loss_float >= loss_threshold:\n",
    "    for iter in range(iterations):\n",
    "        t_train = time_tensor\n",
    "\n",
    "        # forward pass\n",
    "        u_pred = model(t_train)\n",
    "\n",
    "        # calculate loss based on controls that produced by the nn\n",
    "        loss = criterion_fidelity_custom(u_pred, time, Dt)\n",
    "        losses.append(loss.detach().numpy())\n",
    "\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if num_epochs % print_interval == 0:\n",
    "            print(\"Epoch = \", num_epochs, \", Infidelity = \", loss.clone().detach().numpy())\n",
    "\n",
    "        num_epochs += 1\n",
    "\n",
    "        if losses[-1] < loss_threshold:\n",
    "            break\n",
    "\n",
    "    timing_losses.append(losses)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        u_pred = model(t_train)\n",
    "\n",
    "        Ω_pred = u_pred[:, 0:1]\n",
    "        Δ_pred = u_pred[:, 1:2]\n",
    "\n",
    "        # transform tensors to arrays\n",
    "        omegas = np.array([Ω[0] for Ω in Ω_pred.detach().numpy()])\n",
    "        detunings = np.array([Δ[0] for Δ in Δ_pred.detach().numpy()])\n",
    "\n",
    "        controls_array.append((omegas, detunings))\n",
    "\n",
    "        states = get_states(u_pred, time, Dt)\n",
    "\n",
    "        population1 = [(abs(state[0])**2).item() for state in states]\n",
    "        population2 = [(abs(state[1])**2).item() for state in states]\n",
    "        population3 = [(abs(state[2])**2).item() for state in states]\n",
    "\n",
    "        target_state = torch.from_numpy(np.array([0. + 0j, 1., 0]))\n",
    "        fidelities = [abs(torch.inner(state, target_state)) ** 2 for state in states]\n",
    "        final_state = states[-1]\n",
    "        final_states_array.append(final_state)\n",
    "\n",
    "        fidelities_array.append(fidelities)\n",
    "        populations_array.append((population1, population2, population3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the controls, the populations, the fidelity and the loss function calues during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "gs = fig.add_gridspec(2, 2)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "fig.set_figheight(8)\n",
    "fig.set_figwidth(12)\n",
    "\n",
    "axes = [ax1, ax2, ax3, ax4]\n",
    "subplot_params = [(.17, .65, .1, .1), (.60, .65, .1, .1), (.17, .14, .1, .1), (.60, .14, .1, .1)]\n",
    "titles = ['a', 'b', 'c', 'd']\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    omegas, detunings = controls_array[i]\n",
    "    time = timings_array[i]\n",
    "    max_control = max_control_array[i]\n",
    "    \n",
    "    ax.plot(time, omegas, label = 'Ω')\n",
    "    ax.plot(time, detunings, label = 'Δ')\n",
    "    ax.set_ylabel(r\"Ω,Δ ($\\xi$)\", rotation = 90)\n",
    "    ax.set_xlabel(r\"t ($\\xi^{-1}$)\")\n",
    "    ax.set_ylim((-(max_control + 0.1), (max_control + 0.1)))\n",
    "    ax.set_title('(' + titles[i] + ')', loc = \"right\", fontsize = 10)\n",
    "    ax.legend()\n",
    "\n",
    "    l, b, h, w = subplot_params[i]\n",
    "    fidelities = fidelities_array[i]\n",
    "    ax5 = fig.add_axes([l, b, w, h])\n",
    "    ax5.plot(time, fidelities)\n",
    "    ax5.set_ylabel(\"Fidelity\", rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_states_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[torch.abs(state)**2 for state in final_states_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidelities = fidelities_array[0]\n",
    "population1, population2, population3 = populations_array[0]\n",
    "losses = timing_losses[0]\n",
    "omegas, detunings = controls_array[0]\n",
    "time = timings_array[0]\n",
    "\n",
    "fig = plt.figure()\n",
    "gs = fig.add_gridspec(2, 2)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax4 = fig.add_subplot(gs[1, :-1])\n",
    "fig.set_figheight(8)\n",
    "fig.set_figwidth(12)\n",
    "\n",
    "ax1.plot(time, omegas, label = 'Ω')\n",
    "ax1.plot(time, detunings, label = 'Δ')\n",
    "ax1.set_ylabel(r\"Ω,Δ ($\\xi$)\", rotation = 90)\n",
    "ax1.set_xlabel(r\"t ($\\xi^{-1}$)\")\n",
    "# ax1.set_ylim((-1.1, 1.1))\n",
    "ax1.legend()\n",
    "ax1.set_title('(a)', loc = \"right\", fontsize = 10)\n",
    "\n",
    "ax2.plot(time, fidelities)\n",
    "ax2.axhline(y = 0.9999, color = 'r', linestyle = '--', label = '0.9999')\n",
    "ax2.set_ylabel(\"Fidelity\", rotation = 90, fontsize = 12)\n",
    "ax2.set_xlabel(r\"t ($\\xi^{-1}$)\")\n",
    "ax2.legend(loc = 'lower right')\n",
    "ax1.set_title('(b)', loc = \"right\", fontsize = 10)\n",
    "\n",
    "ax4.plot(time, population1, label = r\"$P_1$\")\n",
    "ax4.plot(time, population2, label = r\"$P_2$\")\n",
    "ax4.plot(time, population3, label = r\"$P_3$\")\n",
    "ax4.set_ylabel(\"Populations\", rotation = 90, fontsize = 12)\n",
    "ax4.set_xlabel(r\"t ($\\xi^{-1}$)\")\n",
    "ax4.legend()\n",
    "ax4.set_title('(c)', loc = \"right\", fontsize = 10)\n",
    "\n",
    "epochs_series = np.arange(0, len(losses), 1, dtype = np.int32)\n",
    "ax3.plot(epochs_series, losses, linewidth=2.0)\n",
    "ax3.set_ylabel(\"Loss\")\n",
    "ax3.set_xlabel(\"epochs\")\n",
    "ax3.set_yscale(\"log\")\n",
    "ax3.set_title('(d)', loc = \"right\", fontsize = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
